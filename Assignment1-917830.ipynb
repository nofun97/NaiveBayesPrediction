{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # The University of Melbourne, School of Computing and Information Systems\n",
    "  # COMP30027 Machine Learning, 2019 Semester 1\n",
    "  -----\n",
    "  ## Project 1: Gaining Information about Naive Bayes\n",
    "  -----\n",
    "  ###### Student Name(s): Novan Allanadi\n",
    "  ###### Python version: 3.6\n",
    "  ###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import os\n",
    "import random\n",
    "from fractions import Fraction\n",
    "from math import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute:\n",
    "    '''\n",
    "    Attribute represents a feature of a dataset and contains the types of\n",
    "    values and the frequency of it\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # stores types of values\n",
    "        self.valueCount = 0\n",
    "\n",
    "        # stores types of values and frequency\n",
    "        self.values = {}\n",
    "\n",
    "    def addValue(self, value):\n",
    "        if value == '?':\n",
    "            return\n",
    "        # adding new values to dictionary\n",
    "        if value in self.values:\n",
    "            self.values[value] += 1\n",
    "        else:\n",
    "            self.values[value] = 1\n",
    "            self.valueCount += 1\n",
    "\n",
    "    def addZeroValue(self, value):\n",
    "        # initializing values qith zero frequency\n",
    "        if value not in self.values and value != '?':\n",
    "            self.values[value] = 0\n",
    "\n",
    "    def getValues(self):\n",
    "        # returning all types of values\n",
    "        return self.values.keys()\n",
    "\n",
    "    def getFrequency(self, attr):\n",
    "        # return frequency of a value\n",
    "        if attr not in self.values:\n",
    "            return 0\n",
    "        return self.values[attr]\n",
    "\n",
    "    def getValFreq(self):\n",
    "        # return the dictionary\n",
    "        return self.values\n",
    "\n",
    "    def getNumOfValues(self):\n",
    "        # return number of types of values\n",
    "        return self.valueCount\n",
    "\n",
    "    def getTotalValues(self):\n",
    "        # get total of frequency from all values\n",
    "        total = 0\n",
    "        for i in self.values:\n",
    "            total += self.values[i]\n",
    "        return total\n",
    "\n",
    "\n",
    "class Classifications:\n",
    "    '''\n",
    "    Classifications hold the types of classifications and stores attributes\n",
    "    based on the classification. It also holds the frequency of the\n",
    "    classification and the whole attributes data.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, number, totalData):\n",
    "        # the number of attributes of the datasets\n",
    "        self.numberOfAttributes = number\n",
    "\n",
    "        # stores types of classifications and the corresponding attributes data\n",
    "        self.classifications = {}\n",
    "\n",
    "        # stores the total number of data in the datasets\n",
    "        self.totalNumber = 0\n",
    "\n",
    "        # stores the attributes data without binding it to a classification\n",
    "        self.globalAttributes = []\n",
    "\n",
    "        # initialize global attributes\n",
    "        self.initGlobalAttributes(number, totalData)\n",
    "\n",
    "        # stores the frequency of each classifcation\n",
    "        self.size = {}\n",
    "\n",
    "    def initGlobalAttributes(self, number, totalData):\n",
    "        # create Attribute objects\n",
    "        for _ in range(self.numberOfAttributes):\n",
    "            self.globalAttributes.append(Attribute())\n",
    "\n",
    "        # initialize all types of all attributes using all data\n",
    "        for data in totalData:\n",
    "            for i in range(len(data) - 1):\n",
    "                self.globalAttributes[i].addZeroValue(data[i])\n",
    "\n",
    "    def addNewClassification(self, classification):\n",
    "        if classification == '?':\n",
    "            return\n",
    "        # adding new types of classification\n",
    "        self.classifications[classification] = []\n",
    "        self.size[classification] = 0\n",
    "        for _ in range(self.numberOfAttributes):\n",
    "            self.classifications[classification].append(Attribute())\n",
    "\n",
    "    def addNewDatas(self, datas, classification):\n",
    "        if classification == '?':\n",
    "            return\n",
    "        # adding new attribute data to each classification\n",
    "        if classification not in self.classifications:\n",
    "            self.addNewClassification(classification)\n",
    "\n",
    "        # add total number of data and frequency of each classification\n",
    "\n",
    "        self.totalNumber += 1 if classification != '?' else 0\n",
    "        self.size[classification] += 1\n",
    "        for i in range(self.numberOfAttributes):\n",
    "            cleanedData = datas[i].rstrip('\\n')\n",
    "            # ignoring missing data\n",
    "            if cleanedData == '?':\n",
    "                continue\n",
    "\n",
    "            # adding frequency of the attributes to global and based on\n",
    "            # classification\n",
    "            self.globalAttributes[i].addValue(cleanedData)\n",
    "            self.classifications[classification][i].addValue(cleanedData)\n",
    "\n",
    "    def getClassifications(self):\n",
    "        # get all types of classifications\n",
    "        return self.classifications\n",
    "\n",
    "    def getTotalNumber(self):\n",
    "        # get total number of data\n",
    "        return self.totalNumber\n",
    "\n",
    "    def getTotalNumberOfClassification(self, classification):\n",
    "        # return the frequency of a classification\n",
    "        return self.size[classification]\n",
    "\n",
    "    def getGlobalAttributeData(self, index):\n",
    "        # return the global attribute data\n",
    "        return self.globalAttributes[index]\n",
    "\n",
    "    def getClassificationTypes(self):\n",
    "        # return all types of classifications\n",
    "        return self.classifications.keys()\n",
    "\n",
    "    def getAttributeDataIf(self, classification, index):\n",
    "        # return attribute data corresponding to a classification\n",
    "        return self.classifications[classification][index]\n",
    "\n",
    "    def getNumberOfAttributes(self):\n",
    "        # get number of attributes\n",
    "        return self.numberOfAttributes\n",
    "\n",
    "    def fixClassifications(self):\n",
    "        # adding zero frequency to a type of an attribute should during\n",
    "        # processing, a type of an attribute is not found for a\n",
    "        # classification\n",
    "        for i in range(self.numberOfAttributes):\n",
    "            types = self.globalAttributes[i].getValues()\n",
    "            for c in self.classifications:\n",
    "                for t in types:\n",
    "                    self.classifications[c][i].addZeroValue(t)\n",
    "\n",
    "    def calculateClassEntropy(self):\n",
    "        entropy = 0\n",
    "        for c in self.getClassificationTypes():\n",
    "            # using frequency of a class divided by total number of data\n",
    "            freqRatio = Fraction(self.size[c], self.getTotalNumber())\n",
    "            entropy += freqRatio * log(freqRatio, 2)\n",
    "        return -1 * entropy\n",
    "\n",
    "    def calculateTotalFreqAttrType(self, attrIndex, attrType):\n",
    "        # Calculating total frequency of a certain attribute value\n",
    "        total = 0\n",
    "\n",
    "        # adding all the frequency of an attribute type from each class\n",
    "        for c in self.getClassificationTypes():\n",
    "            attr = self.getAttributeDataIf(c, attrIndex)\n",
    "            total += attr.getFrequency(attrType)\n",
    "\n",
    "        return total\n",
    "\n",
    "    def calculateAttrValueEntropy(self, attrIndex, attrType):\n",
    "        entropy = 0\n",
    "        totalAttrType = self.calculateTotalFreqAttrType(attrIndex, attrType)\n",
    "\n",
    "        # ignoring attribute with zero frequency\n",
    "        if totalAttrType == 0:\n",
    "            return 0\n",
    "\n",
    "        # calculating for each classification\n",
    "        for c in self.getClassificationTypes():\n",
    "            freq = self.getAttributeDataIf(c, attrIndex).getFrequency(attrType)\n",
    "            if freq == 0:\n",
    "                continue\n",
    "\n",
    "            # frequency of a class divided by total attribute type frequency\n",
    "            valFreqRatio = Fraction(freq, totalAttrType)\n",
    "            entropy += valFreqRatio * log(valFreqRatio, 2)\n",
    "\n",
    "        return -1 * entropy\n",
    "\n",
    "    def calculateMeanInfo(self, attrIndex):\n",
    "        globalAttr = self.getGlobalAttributeData(attrIndex)\n",
    "        totalFreq = self.getTotalNumber()\n",
    "        meanInfo = 0\n",
    "\n",
    "        # calculating for each value of an attribute\n",
    "        for t in globalAttr.getValues():\n",
    "            freq = self.calculateTotalFreqAttrType(attrIndex, t)\n",
    "\n",
    "            # frequency of an attribute type divided by total number of data\n",
    "            freqRatio = Fraction(freq, totalFreq)\n",
    "            entropy = self.calculateAttrValueEntropy(attrIndex, t)\n",
    "            meanInfo += freqRatio * entropy\n",
    "\n",
    "        return meanInfo\n",
    "\n",
    "    def calculateInfoGain(self):\n",
    "        infoGain = []\n",
    "        classEntropy = self.calculateClassEntropy()\n",
    "\n",
    "        # for each attribute, calculate information gain\n",
    "        for i in range(self.getNumberOfAttributes()):\n",
    "            mi = self.calculateMeanInfo(i)\n",
    "            infoGain.append(classEntropy - mi)\n",
    "        return infoGain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format\n",
    "\n",
    "def preprocess(datas, totalData):\n",
    "    classifications = None\n",
    "    attrCount = 0\n",
    "\n",
    "    for data in datas:\n",
    "        # initializing classification\n",
    "        if attrCount == 0:\n",
    "            attrCount = len(data)\n",
    "            classifications = Classifications(attrCount - 1, totalData)\n",
    "\n",
    "        if len(data) != attrCount:\n",
    "            raise Exception(\n",
    "                'All data should have the same number of attributes. The line is: {}'.format(data))\n",
    "\n",
    "        # adding new data\n",
    "        classification = data[-1].rstrip('\\n')\n",
    "        classifications.addNewDatas(data[:-1], classification)\n",
    "\n",
    "    # fixing missing values of the classification\n",
    "    classifications.fixClassifications()\n",
    "    return classifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    '''\n",
    "    The Learner class contains the necessary probabilities and all the\n",
    "    calculations required to implement a Naive Bayes Classifier\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Stores the probability of a classification\n",
    "        self.classProbability = {}\n",
    "\n",
    "        # Stores the probability of each types of attribute given a type of\n",
    "        # classification\n",
    "        self.attrProbabilityIf = {}\n",
    "\n",
    "    def learn(self, classification):\n",
    "        # Calculating the probability necessary for the classifier\n",
    "        for t in classification.getClassificationTypes():\n",
    "            # probability of a classification\n",
    "            self.classProbability[t] = Fraction(\n",
    "                classification.getTotalNumberOfClassification(t),\n",
    "                classification.getTotalNumber())\n",
    "\n",
    "            # calculate attribute probability for a given classification\n",
    "            data = []\n",
    "            for i in range(classification.getNumberOfAttributes()):\n",
    "                attr = classification.getAttributeDataIf(t, i)\n",
    "                data.append(self.calculateProbabilities(attr))\n",
    "\n",
    "            self.attrProbabilityIf[t] = data\n",
    "\n",
    "    def getProbabilityIf(self, classification, attr, val):\n",
    "        # returning a neutral value for a missing values\n",
    "        if val == '?':\n",
    "            return 1\n",
    "\n",
    "        # returning a probability given certain classification for a\n",
    "        # certain attribute\n",
    "        return self.attrProbabilityIf[classification][attr][val]\n",
    "\n",
    "    def getClassificationProbability(self, classification):\n",
    "        # get probability for a classification\n",
    "        return self.classProbability[classification]\n",
    "\n",
    "    def predict(self, data, classification):\n",
    "        # predict a classification given certain datas\n",
    "\n",
    "        # get all probabilities given a classification\n",
    "        possibilities = self.getProbabilityGivenData(data)\n",
    "        classification = \"\"\n",
    "        currentProbability = 0\n",
    "\n",
    "        # find a classification with the highest probability\n",
    "        for (k, v) in possibilities.items():\n",
    "            if v > currentProbability:\n",
    "                classification = k\n",
    "                currentProbability = v\n",
    "\n",
    "        return classification\n",
    "\n",
    "    def getProbabilityGivenData(self, data):\n",
    "        # iterate through the data and calculate probability\n",
    "        possibilities = {}\n",
    "        for c in self.classProbability.keys():\n",
    "            possibilities[c] = 1\n",
    "\n",
    "            # multiplying probabilities\n",
    "            for i in range(len(data)):\n",
    "                possibilities[c] *= self.getProbabilityIf(c, i, data[i])\n",
    "            possibilities[c] *= self.getClassificationProbability(c)\n",
    "\n",
    "        return possibilities\n",
    "\n",
    "    def calculateProbabilities(self, attribute):\n",
    "        # calculate probabilities for each attribute given a classification\n",
    "        dict = {}\n",
    "        total = attribute.getTotalValues()\n",
    "        flag = False\n",
    "\n",
    "        # calculating the frequency and total frequency of attributes\n",
    "        for a in attribute.getValues():\n",
    "            value = attribute.getFrequency(a)\n",
    "            dict[a] = (value, total)\n",
    "            if value == 0 and total != 0:\n",
    "                flag = True\n",
    "\n",
    "        # should there be a zero frequency, do probabilistic smoothing\n",
    "        if flag:\n",
    "            dict = self.probabilisticSmoothing(\n",
    "                dict, attribute.getNumOfValues())\n",
    "\n",
    "        # converting the tuples into Fraction\n",
    "        for (k, v) in dict.items():\n",
    "            freq, totalFreq = v\n",
    "            if totalFreq == 0:\n",
    "                dict[k] = 0\n",
    "                continue\n",
    "            dict[k] = Fraction(freq, totalFreq)\n",
    "\n",
    "        return dict\n",
    "\n",
    "    def probabilisticSmoothing(self, probabilities, numOfValues):\n",
    "        # probabilistic smoothing to handle zero frequency\n",
    "        dict = {}\n",
    "        for (k, v) in probabilities.items():\n",
    "            freq, totalFreq = v\n",
    "            dict[k] = (1 + freq, numOfValues + totalFreq)\n",
    "\n",
    "        return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "def train(classifications):\n",
    "    learner = Learner()\n",
    "    learner.learn(classifications)\n",
    "\n",
    "    return learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model\n",
    "def predict(attributes, learner, classifications):\n",
    "    # calculate prediction based on the given attributes\n",
    "    return learner.predict(attributes, classifications)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context\n",
    "def evaluate(dataTest, dataTrain):\n",
    "    score = 0\n",
    "    total = len(dataTest)\n",
    "\n",
    "    classifications = preprocess(dataTrain, dataTest + dataTrain)\n",
    "    classifier = train(classifications)\n",
    "    for data in dataTest:\n",
    "        classLabel = predict(data[:-1], classifier, classifications)\n",
    "        score += 1 if classLabel == data[-1].rstrip('\\n') else 0\n",
    "\n",
    "    return Fraction(score, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain(classifications):\n",
    "    return classifications.calculateInfoGain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/hypothyroid.csv\n",
      "Score:  0.9516282010749288\n",
      "Info Gain:  [0.004628873031652547, 0.0009139351160850073, 0.0012382074503017315, 0.00014844815831743796, 0.0009985293906336068, 0.0013683791752741592, 0.0005423006444424394, 0.0004350938464638965, 0.0004888757691284829, 0.0008983004044028076, 4.463778824304043e-05, 7.8684698479492e-05, 0.009353710215580346, 0.004075493419623766, 0.005792553705846859, 0.005768288201614624, 0.005744031245602799, 0.002580427555574416]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/primary-tumor.csv\n",
      "Score:  0.4365781710914454\n",
      "Info Gain:  [0.1547421418870596, 0.33536005150555503, 1.0262234265467285, 2.0947714990558133, 0.21246189904816637, 0.0203669388480483, 0.10088123982399111, 0.0678727757044233, 0.22052193470670511, 0.1997614363902529, 0.06714460241010656, 0.06025390884525317, 0.29153013602249356, 0.12715354518198252, 0.2458886814337724, 0.18425767171538476, 0.17014811083887338]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/hepatitis.csv\n",
      "Score:  0.832258064516129\n",
      "Info Gain:  [0.03660746514280977, 0.015265380561918285, 0.014490701150154384, 0.08645063847884216, 0.08322845589007444, 0.013806029835453981, 0.0903522944652434, 0.09049078626122975, 0.058739302370822144, 0.12938741279822152, 0.15163520023638288, 0.10012174391687245, 0.08493296456638777]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/anneal.csv\n",
      "Score:  0.8129175946547884\n",
      "Info Gain:  [0.40908953764451006, 0.0, 0.3060515354289405, 0.051344088764404106, 0.29108220585994726, 0.1471188622809556, 0.2137228803159087, 0.29223544065798446, 0.1261663361036096, 0.14107379163812883, 0.032488406491841815, 0.43517783626288575, 0.03870173274881061, 0.00043760652021185287, 0.03935557414283708, 0.021775078259213876, 0.037997478813511565, 0.03670308136440825, 0.0, 0.11722522630372034, 0.029753745208638938, 0.02704235332867677, 0.0, 0.015604780443500665, 0.13718113252042574, 0.0, 0.0223970898516459, 0.01824168402125048, 0.0, 0.0, 0.0, 0.04323960556514961, 0.03303757117705719, 0.01937886432831948, 0.003958783545891853]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/cmc.csv\n",
      "Score:  0.5057705363204344\n",
      "Info Gain:  [0.07090633894894594, 0.04013859922938412, 0.10173991727554088, 0.009820501434384843, 0.002582332379721608, 0.030474214560266555, 0.032511460053806784, 0.015786455595620197]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/car.csv\n",
      "Score:  0.8738425925925926\n",
      "Info Gain:  [0.09644896916961399, 0.07370394692148596, 0.004485716626632108, 0.2196629633399082, 0.030008141247605424, 0.26218435655426386]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/breast-cancer.csv\n",
      "Score:  0.7587412587412588\n",
      "Info Gain:  [0.010605956535614136, 0.0020016149737116518, 0.05717112532429669, 0.06899508808988597, 0.08012009687900967, 0.07700985251661441, 0.0024889884332655043, 0.015066622054149992, 0.025819023909141148]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/nursery.csv\n",
      "Score:  0.9026234567901235\n",
      "Info Gain:  [0.07293460750309988, 0.1964492804881155, 0.005572591715219843, 0.011886431475775838, 0.019602025022871672, 0.0043331270252002785, 0.022232616894018342, 0.9587749604699762]\n",
      "\n",
      "\n",
      "Filepath: /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/mushroom.csv\n",
      "Score:  0.9587641555883801\n",
      "Info Gain:  [0.04879670193537311, 0.028590232773772817, 0.03604928297620391, 0.19237948576121966, 0.9060749773839998, 0.014165027250616302, 0.10088318399657026, 0.23015437514804615, 0.41697752341613137, 0.007516772569664321, 0.4001378247172982, 0.2847255992184845, 0.2718944733927464, 0.2538451734622399, 0.24141556652756657, 0.0, 0.0238170161209168, 0.03845266924309054, 0.3180215107935376, 0.4807049176849154, 0.2019580190668524, 0.1568336046050921]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# script to run evaluation on all the datasets\n",
    "\n",
    "def prepFiles(filepath):\n",
    "    # creates a list of instances from a file\n",
    "    f = open(filepath, 'r')\n",
    "    files = []\n",
    "    length = 0\n",
    "    for line in f.readlines():\n",
    "        files.append(line.split(','))\n",
    "        length += 1\n",
    "\n",
    "    f.close()\n",
    "    return files\n",
    "\n",
    "\n",
    "def collectFiles(mainDir):\n",
    "    # walk through directory and get all the csv files\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(mainDir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                files.append(dirpath + filename)\n",
    "    return files\n",
    "\n",
    "# getting the filepaths in a directory\n",
    "files = collectFiles(\n",
    "    \"/Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/\"\n",
    "    )\n",
    "for fp in files:\n",
    "    # printing the filepath, scores, and info gain for each file\n",
    "    data = prepFiles(fp)\n",
    "    classifications = preprocess(data, data)\n",
    "    print(\"Filepath: \" + fp)\n",
    "    print(\"Score: \", float(evaluate(data, data)))\n",
    "    print(\"Info Gain: \", info_gain(classifications))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Questions (you may respond in a cell or cells below):\n",
    "\n",
    "  1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "  2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "  3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "  4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "  5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "  6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "  Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores (Using training data as test data)\n",
    "#### hypothyroid.csv\n",
    "Score:  0.9516282010749288\n",
    "\n",
    "Info Gain:  [0.004628873031652547, 0.0009139351160850073, 0.0012382074503017315, 0.00014844815831743796, 0.0009985293906336068, 0.0013683791752741592, 0.0005423006444424394, 0.0004350938464638965, 0.0004888757691284829, 0.0008983004044028076, 4.463778824304043e-05, 7.8684698479492e-05, 0.009353710215580346, 0.004075493419623766, 0.005792553705846859, 0.005768288201614624, 0.005744031245602799, 0.002580427555574416]\n",
    "\n",
    "#### primary-tumor.csv\n",
    "\n",
    "Score:  0.4365781710914454\n",
    "\n",
    "Info Gain:  [0.1547421418870596, 0.33536005150555503, 1.0262234265467285, 2.0947714990558133, 0.21246189904816637, 0.0203669388480483, 0.10088123982399111, 0.0678727757044233, 0.22052193470670511, 0.1997614363902529, 0.06714460241010656, 0.06025390884525317, 0.29153013602249356, 0.12715354518198252, 0.2458886814337724, 0.18425767171538476, 0.17014811083887338]\n",
    "\n",
    "\n",
    "#### hepatitis.csv\n",
    "\n",
    "Score:  0.832258064516129\n",
    "\n",
    "Info Gain:  [0.03660746514280977, 0.015265380561918285, 0.014490701150154384, 0.08645063847884216, 0.08322845589007444, 0.013806029835453981, 0.0903522944652434, 0.09049078626122975, 0.058739302370822144, 0.12938741279822152, 0.15163520023638288, 0.10012174391687245, 0.08493296456638777]\n",
    "\n",
    "\n",
    "#### anneal.csv\n",
    "Score:  0.8129175946547884\n",
    "\n",
    "Info Gain:  [0.40908953764451006, 0.0, 0.3060515354289405, 0.051344088764404106, 0.29108220585994726, 0.1471188622809556, 0.2137228803159087, 0.29223544065798446, 0.1261663361036096, 0.14107379163812883, 0.032488406491841815, 0.43517783626288575, 0.03870173274881061, 0.00043760652021185287, 0.03935557414283708, 0.021775078259213876, 0.037997478813511565, 0.03670308136440825, 0.0, 0.11722522630372034, 0.029753745208638938, 0.02704235332867677, 0.0, 0.015604780443500665, 0.13718113252042574, 0.0, 0.0223970898516459, 0.01824168402125048, 0.0, 0.0, 0.0, 0.04323960556514961, 0.03303757117705719, 0.01937886432831948, 0.003958783545891853]\n",
    "\n",
    "#### cmc.csv\n",
    "Score:  0.5057705363204344\n",
    "\n",
    "Info Gain:  [0.07090633894894594, 0.04013859922938412, 0.10173991727554088, 0.009820501434384843, 0.002582332379721608, 0.030474214560266555, 0.032511460053806784, 0.015786455595620197]\n",
    "\n",
    "\n",
    "#### car.csv\n",
    "Score:  0.8738425925925926\n",
    "\n",
    "Info Gain:  [0.09644896916961399, 0.07370394692148596, 0.004485716626632108, 0.2196629633399082, 0.030008141247605424, 0.26218435655426386]\n",
    "\n",
    "\n",
    "#### breast-cancer.csv\n",
    "Score:  0.7587412587412588\n",
    "\n",
    "Info Gain:  [0.010605956535614136, 0.0020016149737116518, 0.05717112532429669, 0.06899508808988597, 0.08012009687900967, 0.07700985251661441, 0.0024889884332655043, 0.015066622054149992, 0.025819023909141148]\n",
    "\n",
    "#### nursery.csv\n",
    "Score:  0.9026234567901235\n",
    "\n",
    "Info Gain:  [0.07293460750309988, 0.1964492804881155, 0.005572591715219843, 0.011886431475775838, 0.019602025022871672, 0.0043331270252002785, 0.022232616894018342, 0.9587749604699762]\n",
    "\n",
    "\n",
    "#### mushroom.csv\n",
    "Score:  0.9587641555883801\n",
    "\n",
    "Info Gain:  [0.04879670193537311, 0.028590232773772817, 0.03604928297620391, 0.19237948576121966, 0.9060749773839998, 0.014165027250616302, 0.10088318399657026, 0.23015437514804615, 0.41697752341613137, 0.007516772569664321, 0.4001378247172982, 0.2847255992184845, 0.2718944733927464, 0.2538451734622399, 0.24141556652756657, 0.0, 0.0238170161209168, 0.03845266924309054, 0.3180215107935376, 0.4807049176849154, 0.2019580190668524, 0.1568336046050921]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Number 1\n",
    "  Yes, it can. Information Gain is a way to measure how important an attribute\n",
    "  is to make predictions based on a set of data. The high value of an attribute's Information Gain means the attribute affects the class of an instance more. In the\n",
    "  given datasets, it can be seen that, if a classifier has more attributes with\n",
    "  relatively high Information Gain, the classifier tends to be more accurate.\n",
    "  However, there are outliers such as the hypothyroid.csv and\n",
    "  primary-tumor.csv.\n",
    "  In the hypothyroid.csv, Information Gain of each attribute is relatively low, and it should make the classifier less accurate. But, the accuracy is relatively high. This can be explained due to the high count of \"negative\" cases of hypothyroid in the dataset. This makes the classifier tend to predict \"negative\" which is true in most of the data. Due to this, Information Gain affects classifier prediction less.\n",
    "  In the primary-tumor.csv, the Information Gain of several attributes are relatively high, and with such values, the classifier should be able to make more accurate predictions. However, the accuracy is, in fact, the lowest. This is due to the high count of missing values in the dataset. Due to this, the value of Information Gain can be high as the missing values make the Mean Information smaller. This also makes it harder for the classifier to predict as the high count of missing values contributes to the problem of lack of data. Hence, the low accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/hypothyroid.csv\n",
      "Score:  0.9519595448798989\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/primary-tumor.csv\n",
      "Score:  0.31176470588235294\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/hepatitis.csv\n",
      "Score:  0.8205128205128205\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/anneal.csv\n",
      "Score:  0.5657015590200446\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/cmc.csv\n",
      "Score:  0.47761194029850745\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/car.csv\n",
      "Score:  0.8599537037037037\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/breast-cancer.csv\n",
      "Score:  0.6783216783216783\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/nursery.csv\n",
      "Score:  0.903858024691358\n",
      "\n",
      "\n",
      "Filepath:  /Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/mushroom.csv\n",
      "Score:  0.9451009354997538\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Holdout implementation for question number 4\n",
    "def holdout(trainPercentage, filepath):\n",
    "    files = prepFiles(filepath)\n",
    "\n",
    "    # if asked for all data, return the same data as the test data and train\n",
    "    # data\n",
    "    if trainPercentage == 1:\n",
    "        return files, files\n",
    "\n",
    "    # getting the number of training data\n",
    "    trainLength = int(len(files) * trainPercentage)\n",
    "\n",
    "    # shuffling the files so that each instance is randomly assigned as either\n",
    "    # training data or test data\n",
    "    random.shuffle(files)\n",
    "\n",
    "    return files[:trainLength], files[trainLength:]\n",
    "\n",
    "# grab all the filepaths in the directory\n",
    "files = collectFiles(\n",
    "    \"/Users/novan/Desktop/CODE/Machine Learning/assignment1/2019S1-proj1-data/\"\n",
    "    )\n",
    "\n",
    "# train percentage to determine the percentage of training data\n",
    "trainPercentage = 0.5\n",
    "\n",
    "# split and evaluate data, then print the necessary data\n",
    "for fp in files:\n",
    "    trainData, testData = holdout(trainPercentage, fp)\n",
    "    print(\"Filepath: \", fp)\n",
    "    print(\"Score: \", float(evaluate(testData, trainData)))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number 4\n",
    "The holdout implementation takes in a percentage of data, in this case 80 % - 20 % and 50 % - 50 % of training data - test data. The data is shuffled so that each instance is randomly assigned as either training data or test data.\n",
    "\n",
    "By splitting the data into training data and testing data, it is expected that the scores by implementing holdout evaluation would be lower than the test that uses training data as testing data. In this case, by comparing each score, it can be concluded that the expectation was correct. Most of the accuracy score tend to be lower than the score with the classifier that uses all data for training and testing. Some of the differences are small, with only 0.1 % to 1 % of difference, but there are classifiers that implement holdout with an even lower score, even as low as 15 % (the primary-tumor.csv 50-50). There are of course some cases where the classifier scores higher, but the number of cases is smaller than the lower score cases. The difference is relatively insignificant. The most significant one would be the 80-20 hepatitis.csv classifier with the difference of 6 % with the classifier that uses all of the data as training data. This might be due to the lack of data which influence the probability calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Holdout 80% as training data\n",
    "\n",
    "#### hypothyroid.csv\n",
    "\n",
    "Score:  0.957345971563981\n",
    "\n",
    "Higher by 0.6%\n",
    "\n",
    "\n",
    "#### primary-tumor.csv\n",
    "\n",
    "Score:  0.3088235294117647\n",
    "\n",
    "Lower by 13%\n",
    "\n",
    "\n",
    "#### hepatitis.csv\n",
    "\n",
    "Score:  0.8387096774193549\n",
    "\n",
    "Higher by 6%\n",
    "\n",
    "\n",
    "#### anneal.csv\n",
    "\n",
    "Score:  0.7722222222222223\n",
    "\n",
    "Lower by 4%\n",
    "\n",
    "\n",
    "#### cmc.csv\n",
    "\n",
    "Score:  0.46779661016949153\n",
    "\n",
    "Lower by 4%\n",
    "\n",
    "\n",
    "#### car.csv\n",
    "\n",
    "Score:  0.8323699421965318\n",
    "\n",
    "Lower by 4%\n",
    "\n",
    "\n",
    "#### breast-cancer.csv\n",
    "\n",
    "Score:  0.7931034482758621\n",
    "\n",
    "Higher by 4%\n",
    "\n",
    "\n",
    "#### nursery.csv\n",
    "\n",
    "Score:  0.9012345679012346\n",
    "\n",
    "Lower by 0.1%\n",
    "\n",
    "\n",
    "#### mushroom.csv\n",
    "\n",
    "Score:  0.9587692307692308\n",
    "\n",
    "Very close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score with Holdout 50-50\n",
    "#### hypothyroid.csv\n",
    "\n",
    "Score:  0.9494310998735778\n",
    "\n",
    "Lower by 0.2%\n",
    "\n",
    "\n",
    "#### primary-tumor.csv\n",
    "\n",
    "Score:  0.2823529411764706\n",
    "\n",
    "Lower by 15%\n",
    "\n",
    "\n",
    "#### hepatitis.csv\n",
    "\n",
    "Score:  0.8333333333333334\n",
    "\n",
    "Higher by 0.1%\n",
    "\n",
    "\n",
    "#### anneal.csv\n",
    "\n",
    "Score:  0.8106904231625836\n",
    "\n",
    "Lower by 0.2%\n",
    "\n",
    "\n",
    "#### cmc.csv\n",
    "\n",
    "Score:  0.49525101763907736\n",
    "\n",
    "Lower by 1%\n",
    "\n",
    "\n",
    "#### car.csv\n",
    "\n",
    "Score:  0.8460648148148148\n",
    "\n",
    "Lower by 3%\n",
    "\n",
    "\n",
    "#### breast-cancer.csv\n",
    "\n",
    "Score:  0.7622377622377622\n",
    "\n",
    "Higher by 1%\n",
    "\n",
    "\n",
    "#### nursery.csv\n",
    "\n",
    "Score:  0.8976851851851851\n",
    "\n",
    "Lower by 1%\n",
    "\n",
    "\n",
    "#### mushroom.csv\n",
    "\n",
    "Score:  0.9455933037912359\n",
    "\n",
    "Lower by 1%\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
